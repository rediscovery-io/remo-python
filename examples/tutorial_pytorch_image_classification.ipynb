{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "<a href=\"https://raw.githubusercontent.com/rediscovery-io/remo-python/master/examples/tutorial_pytorch_image_classification.ipynb\" target=\"_blank\"><button style=\"background-image: url('https://raw.githubusercontent.com/rediscovery-io/remo-python/master/examples/assets/download.svg');height: 20px; width:117px;border: none; outline: none;)\"></button> <a href=\"http://colab.research.google.com/github/rediscovery-io/remo-python/blob/master/examples/google-colab/tutorial_pytorch_image_classification.ipynb\" target=\"_blank\"><button style=\"background-image: url('https://colab.research.google.com/assets/colab-badge.svg');height: 20px; width:117px;border: none; outline: none;)\"></button>\n",
    "</div>\n",
    "\n",
    "# Image Classification Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, we will use Remo to speed up the process of building a transfer learning pipeline for an Image Classification task.** \n",
    "\n",
    "\n",
    "In particular, we will:\n",
    "\n",
    "- Use Remo to visualize and explore our images and annotations\n",
    "- Use Remo to quickly access some key statistics of our Dataset\n",
    "- Create custom train/test/val splits in PyTorch without needing to move data around (thanks to Remo image tags)\n",
    "- Visually compare our model predictions with our ground-truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EI72wutnsdyO",
    "outputId": "347b3820-f40b-4707-c6c9-666680d951fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "import remo\n",
    "remo.set_viewer('jupyter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "\n",
    "- For this example, our dataset is a subset of the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Flowers 102 Dataset</a>.\n",
    "- In the next cell we download the data as a zip file and extract the files in a new folder.\n",
    "\n",
    "- The folder structure of the dataset is:\n",
    "\n",
    "    ```\n",
    "    ├── small_flowers\n",
    "        ├── images\n",
    "            ├── 0\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── 1\n",
    "                ├── image_3.jpg\n",
    "                ├── image_4.jpg\n",
    "                ├── ...\n",
    "        ├── annotations\n",
    "            ├── images_tags.csv\n",
    "            ├── annotations.csv\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset will be extracted in a new folder\n",
    "if not os.path.exists('small_flowers.zip'):\n",
    "    !wget https://s-3.s3-eu-west-1.amazonaws.com/small_flowers.zip\n",
    "    !unzip -qq small_flowers.zip\n",
    "else:\n",
    "    print('Files already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "path_to_images =  './small_flowers/images/'\n",
    "path_to_annotations = './small_flowers/annotations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "\n",
    "We can easily generate annotations from a series of folders, by passing the root directory path to ```remo.generate_annotations_from_folders(). ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_file_path = os.path.join(path_to_annotations, 'annotations.csv')\n",
    "remo.generate_annotations_from_folders(path_to_data_folder = path_to_images, \n",
    "                                       output_file_path = annotations_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the labels as strings rather than IDs, we can use a dictionary mapping the two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_index = { 0 : 'Pink Primrose',  \n",
    "                 1 : 'Hard-leaved Pocket Orchid', \n",
    "                 2 : 'Canterbury Bells'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Remo, we can use tags to organise our images.\n",
    "Among other things, this allows us to generate train / test splits without the need to move image files around.\n",
    "\n",
    "To do this, we just need to pass a dictionary (mapping tags to the relevant images paths) to the function \n",
    "```remo.generate_image_tags()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_list = [os.path.abspath(i) for i in glob.glob(path_to_images + '/**/*.jpg', recursive=True)]\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "# Definining the train test split\n",
    "train_idx = round(len(im_list) * 0.8)\n",
    "valid_idx = train_idx + round(len(im_list) * 0.1)\n",
    "test_idx  = valid_idx + round(len(im_list) * 0.1)\n",
    "\n",
    "# Creating a dictionary with tags\n",
    "tags_dict =  {'train' : im_list[0:train_idx], \n",
    "              'valid' : im_list[train_idx:valid_idx], \n",
    "              'test' : im_list[valid_idx:test_idx]}\n",
    "\n",
    "train_test_split_file_path = os.path.join(path_to_annotations, 'images_tags.csv') \n",
    "remo.generate_image_tags(tags_dictionary  = tags_dict, \n",
    "                         output_file_path = train_test_split_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "To create a dataset we can use ```remo.create_dataset()```, specifying the path to data and annotations.\n",
    "\n",
    "The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported, you can <a href=\"https://remo.ai/docs/annotation-formats/\"> refer to the docs</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "flowers =  remo.create_dataset(name = 'flowers', \n",
    "                              local_files = [path_to_images, path_to_annotations],\n",
    "                              annotation_task = 'Image classification',\n",
    "                              class_encoding = cat_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view and explore images and labels, we can use Remo directly from the notebook. We just need to call ```dataset.view()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_view](assets/flower_dataset_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties. \n",
    "\n",
    "This can be done either using code, or via the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'AnnotationSet ID': 348,\n  'AnnotationSet name': 'Image classification',\n  'n_images': 140,\n  'n_classes': 3,\n  'n_objects': 0,\n  'top_3_classes': [{'name': 'Hard-leaved pocket orchid', 'count': 60},\n   {'name': 'Canterbury bells', 'count': 40},\n   {'name': 'Pink primrose', 'count': 40}],\n  'creation_date': None,\n  'last_modified_date': '2020-09-02T08:30:19.135869Z'},\n {'AnnotationSet ID': 349,\n  'AnnotationSet name': 'model_predictions',\n  'n_images': 14,\n  'n_classes': 3,\n  'n_objects': 0,\n  'top_3_classes': [{'name': 'Hard-leaved pocket orchid', 'count': 7},\n   {'name': 'Pink primrose', 'count': 4},\n   {'name': 'Canterbury bells', 'count': 3}],\n  'creation_date': None,\n  'last_modified_date': '2020-09-02T08:32:51.212628Z'}]"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "flowers.get_annotation_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view_annotations_stats](assets/flower_dataset_view_annotation_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "Here we start working with PyTorch. To load the data, we will define a custom PyTorch ```Dataset``` object (as usual with PyTorch).\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "- **train_test_valid_split (Path to Tags):** path to tags csv file for Train, Test, Validation split. Format: file_name, tag.\n",
    "- **annotations (Path to Annotations):** path to the annotations CSV File. Format : file_name, class_name\n",
    "- **mapping (Mapping):** a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrPCv6E8ipU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset Class to facilitate loading data for the Image Classifcation Task\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations, train_test_valid_split, mapping = None, mode = 'train', transform = None):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            annotations: The path to the annotations CSV file. Format: file_name, class_name\n",
    "            train_test_valid_split: The path to the tags CSV file for train, test, valid split. Format: file_name, tag\n",
    "            mapping: a dictionary containing mapping of class name and class index. Format : {'class_name' : 'class_index'}, Default: None\n",
    "            mode: Mode in which to instantiate class. Default: 'train'\n",
    "            transform: The transforms to be applied to the image data\n",
    "\n",
    "        Returns:\n",
    "            image : Torch Tensor, label_tensor : Torch Tensor, file_name : str\n",
    "        \"\"\"\n",
    "        \n",
    "        my_data = pd.read_csv(annotations, index_col='file_name')\n",
    "        my_data['tag'] = pd.read_csv(train_test_valid_split, index_col='file_name')\n",
    "        my_data = my_data.reset_index()\n",
    "\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        my_data = my_data.loc[my_data['tag'] == mode].reset_index(drop=True)\n",
    "        self.data = my_data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mapping is not None:\n",
    "            labels = int(self.mapping[self.data.loc[idx, 'class_name'].lower()])\n",
    "        else:\n",
    "            labels = int(self.data.loc[idx, 'class_name'])\n",
    "        \n",
    "        im_path = self.data.loc[idx, 'file_name']\n",
    "        \n",
    "        label_tensor =  torch.as_tensor(labels, dtype=torch.long)\n",
    "        im = Image.open(im_path)\n",
    "    \n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "    \n",
    "        if self.mode == 'test':\n",
    "            # For saving the predictions, the file name is required\n",
    "            return {'im' : im, 'labels': label_tensor, 'im_name' : self.data.loc[idx, 'file_name']}\n",
    "        else:\n",
    "            return {'im' : im, 'labels' : label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means =  [0.485, 0.456, 0.406]\n",
    "stds  =  [0.229, 0.224, 0.225]\n",
    "\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms      =  transforms.Compose([\n",
    "                         transforms.RandomRotation(30),\n",
    "                         transforms.RandomResizedCrop(224),\n",
    "                         transforms.RandomHorizontalFlip(p=0.5),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])\n",
    "\n",
    "test_valid_transforms =  transforms.Compose([\n",
    "                         transforms.Resize(224),\n",
    "                         transforms.CenterCrop(224),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FlowerDataset(annotations =  annotations_file_path,\n",
    "                              train_test_valid_split = train_test_split_file_path,\n",
    "                              transform =  train_transforms,\n",
    "                              mode =  'train')\n",
    "\n",
    "valid_dataset = FlowerDataset(annotations = annotations_file_path,\n",
    "                              train_test_valid_split = train_test_split_file_path,\n",
    "                              transform = test_valid_transforms,\n",
    "                              mode = 'valid')\n",
    "\n",
    "test_dataset  = FlowerDataset(annotations = annotations_file_path,\n",
    "                              train_test_valid_split = train_test_split_file_path,\n",
    "                              transform = test_valid_transforms,\n",
    "                              mode = 'test')\n",
    "\n",
    "# If you face issues in operating systems like Windows, you can set num_workers=0.\n",
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=1)\n",
    "val_loader   =  torch.utils.data.DataLoader(valid_dataset, batch_size=1,  shuffle=False, num_workers=1)\n",
    "test_loader  =  torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We use a ```ResNet-18``` architecture, with weight pre-trained on ImageNet.\n",
    "\n",
    "To train the model, the following details are to be specified:\n",
    "\n",
    "1. **Model:** The edited version of the pre-trained model.\n",
    "2. **Data Loaders:** The dictionary containing our training and validation dataloaders\n",
    "3. **Criterion:** The loss function used for training the network\n",
    "4. **Num_epochs:** The number of epochs for which we would like to train the network.\n",
    "5. **dataset_size:** an additional parameter which is used to correctly scale the loss, the method for this is specified in the DataLoader cell\n",
    "6. **num_classes:** Number of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "1e193402cc1b4dbe97531c28ac93a8bb",
      "18d04c743bb4459d8a8235e8846b178e",
      "ec07bf4e45574e9ca39217f19a0adb48",
      "822d18ace03e4637892cf40ae0996e4a",
      "ca38e79005dd4612ae97645c53113ae2",
      "2690eec5fed040cd801e83c2f178d068",
      "b37da2626af94b649d19213f38e14d65",
      "403ac43de7454dd4b4dfdab3a61ffe14"
     ]
    },
    "colab_type": "code",
    "id": "lCxV4MgZ8fEs",
    "outputId": "325236d0-57df-49f3-b881-a81183de8559"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# Freezing the weights\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "\n",
    "# Replacing the final layer\n",
    "model.fc =  nn.Sequential(nn.Linear(512, 256), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(p=0.5), \n",
    "                         nn.Linear(256, num_classes), \n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "optimizer    =  optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "criterion    =  nn.NLLLoss()\n",
    "num_epochs   =  2\n",
    "data_loaders =  {'train' : train_loader, 'valid': val_loader}\n",
    "dataset_size =  {'train' : len(train_dataset), 'valid' : len(valid_dataset)}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# This method pushes the model to the device.\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usNSOE7Ao6n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training loop trains the model for the total number of epochs.\n",
    "# (1 epoch = one complete pass over the entire dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train() # This sets the model back to training after the validation step\n",
    "    print('\\nEpoch Number {}'.format(epoch+1))\n",
    "\n",
    "    training_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0\n",
    "    correct_preds = 0\n",
    "    best_acc = 0\n",
    "    validation = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    train_data_loader = tqdm.tqdm(data_loaders['train'])\n",
    "    \n",
    "    for x, data in enumerate(train_data_loader):\n",
    "        inputs, labels = data['im'].to(device), data['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = training_loss / dataset_size['train']\n",
    "    print('Training Loss : {:.5f}'.format(epoch_loss))\n",
    "    valid_data_loader = tqdm.tqdm(data_loaders['valid'])\n",
    "    \n",
    "    # Validation step after every epoch\n",
    "    # The gradients are not required at inference time, hence the model is set to eval mode\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, data in enumerate(valid_data_loader):\n",
    "            inputs, labels = data['im'].to(device), data['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            _, index = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct_preds += (index == labels).sum().item()\n",
    "\n",
    "            validation += val_loss.item()\n",
    "\n",
    "        val_acc = 100 * (correct_preds / total)\n",
    "\n",
    "        print('Validation Loss : {:.5f}'.format(validation / dataset_size['valid']))\n",
    "        print('Validation Accuracy is: {:.2f}%'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oASN5m6Ibn9l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.eval()\n",
    "test_data_loader =  tqdm.tqdm(test_loader)\n",
    "\n",
    "total =  0\n",
    "correct_preds =  0\n",
    "pred_list =  {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, data in enumerate(test_data_loader):\n",
    "        single_im, label = data['im'].to(device), data['labels'].to(device)\n",
    "        im_name = data['im_name']\n",
    "        \n",
    "        pred = model(single_im)\n",
    "\n",
    "        _, index = torch.max(pred, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct_preds += (index == label).sum().item()\n",
    "        \n",
    "        pred_list[os.path.basename(im_name[0])] = cat_to_index[index.item()]\n",
    "        \n",
    "df = pd.DataFrame(pred_list.items(), columns=['file_name', 'class_name'])\n",
    "\n",
    "model_predictions_path = os.path.join(path_to_annotations, 'model_predictions.csv')\n",
    "\n",
    "with open(model_predictions_path, 'w') as f:\n",
    "    df.to_csv(f, index=False)\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * (correct_preds / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Using Remo, we can visually compare the model predictions against the original labels.\n",
    "\n",
    "To do this we create a new ```AnnotationSet```, and  upload predictions as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = flowers.create_annotation_set(annotation_task='Image Classification', \n",
    "                                            name = 'model_predictions',\n",
    "                                            paths_to_files = [train_test_split_file_path, model_predictions_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results Comparison](assets/flower_dataset_view_predictions.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "remo_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d04c743bb4459d8a8235e8846b178e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e193402cc1b4dbe97531c28ac93a8bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec07bf4e45574e9ca39217f19a0adb48",
       "IPY_MODEL_822d18ace03e4637892cf40ae0996e4a"
      ],
      "layout": "IPY_MODEL_18d04c743bb4459d8a8235e8846b178e"
     }
    },
    "2690eec5fed040cd801e83c2f178d068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "403ac43de7454dd4b4dfdab3a61ffe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822d18ace03e4637892cf40ae0996e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403ac43de7454dd4b4dfdab3a61ffe14",
      "placeholder": "​",
      "style": "IPY_MODEL_b37da2626af94b649d19213f38e14d65",
      "value": " 44.7M/44.7M [00:21&lt;00:00, 2.16MB/s]"
     }
    },
    "b37da2626af94b649d19213f38e14d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca38e79005dd4612ae97645c53113ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec07bf4e45574e9ca39217f19a0adb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2690eec5fed040cd801e83c2f178d068",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca38e79005dd4612ae97645c53113ae2",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}